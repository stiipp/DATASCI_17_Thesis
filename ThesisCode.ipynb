{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e565d4",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c970c6",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "- **File Reading:** Load signal files from the specified directories for healthy and patient groups\n",
    "- **Label Assignment:** Assign a numerical label to each file based on its category (healthy or patient) for supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34f15cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup (Updated channel names)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Base paths configuration\n",
    "base_paths = {\n",
    "    \"healthy\": r\"C:\\NewHandPD\\Healthy Signals\\Signal\",\n",
    "    \"patient\": r\"C:\\NewHandPD\\Patient Signals\\Signal\"\n",
    "}\n",
    "\n",
    "category_labels = {\n",
    "    \"healthy\": 0,\n",
    "    \"patient\": 1\n",
    "}\n",
    "\n",
    "# Updated channel names (excluding Microphone)\n",
    "channel_names = [\n",
    "    \"Fingergrip\", \"Axial_Pressure\",\n",
    "    \"Tilt_X\", \"Tilt_Y\", \"Tilt_Z\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f99b4",
   "metadata": {},
   "source": [
    "## Data Parsing\n",
    "- **Metadata Extraction:** Extract relevant metadata from each file, such as subject information and recording parameters, to enrich the dataset.\n",
    "- **Signal Processing:** Parse and structure the signal data, dropping unnecessary channels (e.g., Microphone) and keeping only the relevant sensor channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "660a4a02-fb71-4284-8fa2-d894b500572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Enhanced File Parsing Function with Metadata Extraction\n",
    "def parse_signal_file(file_path, label):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    meta_info = {}\n",
    "    signal_start_idx = 0\n",
    "    in_meta_section = False\n",
    "\n",
    "    # Extract metadata\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        \n",
    "        if line == \"#<meta>\":\n",
    "            in_meta_section = True\n",
    "            continue\n",
    "        elif line == \"#</meta>\":\n",
    "            in_meta_section = False\n",
    "            signal_start_idx = i + 1\n",
    "            break\n",
    "            \n",
    "        if in_meta_section and line.startswith(\"#<\") and line.endswith(\">\"):\n",
    "            # Extract key-value pairs from metadata\n",
    "            try:\n",
    "                key = line[2:line.find(\">\")]  # Get text between #< and >\n",
    "                value = line[line.find(\">\")+1:-1]  # Get text between > and <\n",
    "                \n",
    "                # Convert numeric values to appropriate type\n",
    "                if value.isdigit():\n",
    "                    value = int(value)\n",
    "                elif value.replace('.', '', 1).isdigit():\n",
    "                    value = float(value)\n",
    "                elif value.lower() == 'true':\n",
    "                    value = True\n",
    "                elif value.lower() == 'false':\n",
    "                    value = False\n",
    "                elif value == '':\n",
    "                    value = None\n",
    "                    \n",
    "                meta_info[key] = value\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # Load signal data and drop first channel (Microphone)\n",
    "    signal_lines = lines[signal_start_idx:]\n",
    "    signal_array = np.loadtxt(signal_lines, delimiter=\"\\t\")\n",
    "    \n",
    "    # Keep only channels 1-5 (drop channel 0 - Microphone)\n",
    "    if signal_array.ndim == 1:\n",
    "        signal_array = signal_array[1:6].reshape(1, -1)  # For single-row signals\n",
    "    else:\n",
    "        signal_array = signal_array[:, 1:6]  # For multi-row signals\n",
    "\n",
    "    return signal_array, label, meta_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "460ddd17-8b78-4fd8-8b78-92829c0bf3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing healthy files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [00:04<00:00, 89.29it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing patient files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 373/373 [00:08<00:00, 46.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Main Processing (Now includes metadata in records)\n",
    "all_records = []\n",
    "sigMea_records = []\n",
    "sigSp_records = []\n",
    "\n",
    "for category, folder_path in base_paths.items():\n",
    "    label = category_labels[category]\n",
    "    print(f\"Processing {category} files...\")\n",
    "\n",
    "    for file_name in tqdm(os.listdir(folder_path)):\n",
    "        if not file_name.endswith(\".txt\"):\n",
    "            continue\n",
    "            \n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            signal_data, label_val, meta_info = parse_signal_file(file_path, label)\n",
    "\n",
    "            record = {\n",
    "                \"file_name\": file_name,\n",
    "                \"label\": label_val,\n",
    "                \"signal\": signal_data,\n",
    "                **meta_info  # Unpack all metadata into the record\n",
    "            }\n",
    "\n",
    "            all_records.append(record)\n",
    "            \n",
    "            # Filter for specific file prefixes\n",
    "            if file_name.startswith(\"sigMea\"):\n",
    "                sigMea_records.append(record)\n",
    "            elif file_name.startswith(\"sigSp\"):\n",
    "                sigSp_records.append(record)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "531aa740-e3ed-49fb-94b5-6215dcb6fdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "Total files processed: 792\n",
      "Files starting with 'sigMea': 264\n",
      "Files starting with 'sigSp': 264\n",
      "\n",
      "Metadata columns found:\n",
      "['Person_ID_Number', 'Age', 'Gender', 'Writing_Hand', 'Weight', 'Height', 'Smoker', 'Notice', 'Object', 'Object_Index', 'Pen', 'Samplerate', 'Time', 'Date', 'Comment', 'Surename', 'Forename']\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create DataFrames (Now includes metadata columns)\n",
    "df_all = pd.DataFrame(all_records)\n",
    "df_sigMea = pd.DataFrame(sigMea_records)\n",
    "df_sigSp = pd.DataFrame(sigSp_records)\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Total files processed: {len(all_records)}\")\n",
    "print(f\"Files starting with 'sigMea': {len(sigMea_records)}\")\n",
    "print(f\"Files starting with 'sigSp': {len(sigSp_records)}\")\n",
    "\n",
    "# Show metadata columns we've extracted\n",
    "print(\"\\nMetadata columns found:\")\n",
    "print([col for col in df_all.columns if col not in ['file_name', 'label', 'signal']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976c85e1",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "- **Missing and Outlier Data Handling:** Remove records with missing values or impossible signal values (e.g., negative physical measurements) to ensure data quality.\n",
    "- **Signal Normalization:** Apply min-max scaling to each signal channel so all features are on a comparable scale, improving model performance and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40c60beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning complete:\n",
      "df_all_clean shape: (789, 20)\n",
      "df_sigMea_clean shape: (262, 20)\n",
      "df_sigSp_clean shape: (263, 20)\n",
      "\n",
      "Missing values in df_all_clean: 1674\n",
      "Missing values in df_sigMea_clean: 556\n",
      "Missing values in df_sigSp_clean: 558\n",
      "\n",
      "Columns with missing values in df_all_clean:\n",
      "Weight       60\n",
      "Height       60\n",
      "Surename    777\n",
      "Forename    777\n",
      "dtype: int64\n",
      "\n",
      "Columns with missing values in df_sigMea_clean:\n",
      "Weight       20\n",
      "Height       20\n",
      "Surename    258\n",
      "Forename    258\n",
      "dtype: int64\n",
      "\n",
      "Columns with missing values in df_sigSp_clean:\n",
      "Weight       20\n",
      "Height       20\n",
      "Surename    259\n",
      "Forename    259\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Data Cleaning for DataFrames\n",
    "# Remove rows with missing or NaN values in key columns\n",
    "df_all_clean = df_all.dropna(subset=['signal', 'label', 'file_name'])\n",
    "df_sigMea_clean = df_sigMea.dropna(subset=['signal', 'label', 'file_name'])\n",
    "df_sigSp_clean = df_sigSp.dropna(subset=['signal', 'label', 'file_name'])\n",
    "\n",
    "# Remove rows with impossible values (e.g., negative values for physical signals)\n",
    "def remove_impossible_values(df):\n",
    "    # Example: Remove rows where any signal value is negative\n",
    "    mask = df['signal'].apply(lambda x: np.all(x >= 0))\n",
    "    return df[mask]\n",
    "\n",
    "df_all_clean = remove_impossible_values(df_all_clean)\n",
    "df_sigMea_clean = remove_impossible_values(df_sigMea_clean)\n",
    "df_sigSp_clean = remove_impossible_values(df_sigSp_clean)\n",
    "\n",
    "# Normalize signal data (min-max scaling per sample)\n",
    "def normalize_signal(signal):\n",
    "    min_val = np.min(signal, axis=0)\n",
    "    max_val = np.max(signal, axis=0)\n",
    "    # Avoid division by zero\n",
    "    denom = np.where(max_val - min_val == 0, 1, max_val - min_val)\n",
    "    return (signal - min_val) / denom\n",
    "\n",
    "df_all_clean['signal'] = df_all_clean['signal'].apply(normalize_signal)\n",
    "df_sigMea_clean['signal'] = df_sigMea_clean['signal'].apply(normalize_signal)\n",
    "df_sigSp_clean['signal'] = df_sigSp_clean['signal'].apply(normalize_signal)\n",
    "\n",
    "print('Data cleaning complete:')\n",
    "print(f'df_all_clean shape: {df_all_clean.shape}')\n",
    "print(f'df_sigMea_clean shape: {df_sigMea_clean.shape}')\n",
    "print(f'df_sigSp_clean shape: {df_sigSp_clean.shape}')\n",
    "\n",
    "# Check for missing values in each DataFrame\n",
    "print(\"\\nMissing values in df_all_clean:\", df_all_clean.isna().sum().sum())\n",
    "print(\"Missing values in df_sigMea_clean:\", df_sigMea_clean.isna().sum().sum())\n",
    "print(\"Missing values in df_sigSp_clean:\", df_sigSp_clean.isna().sum().sum())\n",
    "\n",
    "# To see which columns have missing values:\n",
    "print(\"\\nColumns with missing values in df_all_clean:\")\n",
    "print(df_all_clean.isna().sum()[df_all_clean.isna().sum() > 0])\n",
    "\n",
    "print(\"\\nColumns with missing values in df_sigMea_clean:\")\n",
    "print(df_sigMea_clean.isna().sum()[df_sigMea_clean.isna().sum() > 0])\n",
    "\n",
    "print(\"\\nColumns with missing values in df_sigSp_clean:\")\n",
    "print(df_sigSp_clean.isna().sum()[df_sigSp_clean.isna().sum() > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3194d81d",
   "metadata": {},
   "source": [
    "# Data Splitting\n",
    "- **Person-Based Splitting:** Divide the dataset into training, validation, and test sets based on unique person identifiers to prevent data leakage and ensure fair evaluation.\n",
    "- **Randomization:** Shuffle person IDs before splitting to ensure unbiased distribution across splits.\n",
    "- **Split Proportions:** Assign approximately 70% of persons to training, 15% to validation, and 15% to test sets for balanced model development and assessment.\n",
    "- **Leakage Check:** Verify that no person appears in more than one split to maintain strict separation between training and evaluation data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41f1769b-e204-4cad-af58-fbe7e317cb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame shape: (528, 20)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Combine sigMea and sigSp DataFrames\n",
    "df_combined = pd.concat([df_sigMea, df_sigSp], ignore_index=True)\n",
    "print(f\"Combined DataFrame shape: {df_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a281ba84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Split Summary:\n",
      "Total number of unique persons: 61\n",
      "Training set: 42 persons, 368 samples\n",
      "Validation set: 9 persons, 80 samples\n",
      "Test set: 10 persons, 80 samples\n",
      "\n",
      "Checking for data leakage:\n",
      "Overlap between train and val: 0\n",
      "Overlap between train and test: 0\n",
      "Overlap between val and test: 0\n"
     ]
    }
   ],
   "source": [
    "# Cell 6.5: Split data by Person_ID_Number\n",
    "import numpy as np\n",
    "\n",
    "# Extract unique Person IDs\n",
    "person_ids = df_combined['Person_ID_Number'].unique()\n",
    "\n",
    "# Random shuffle of Person IDs\n",
    "np.random.seed(42)  # for reproducibility\n",
    "np.random.shuffle(person_ids)\n",
    "\n",
    "# Calculate split indices\n",
    "n_ids = len(person_ids)\n",
    "train_idx = int(0.7 * n_ids)\n",
    "val_idx = int(0.85 * n_ids)\n",
    "\n",
    "# Split Person IDs into train, validation, and test sets\n",
    "train_ids = person_ids[:train_idx]\n",
    "val_ids = person_ids[train_idx:val_idx]\n",
    "test_ids = person_ids[val_idx:]\n",
    "\n",
    "# Create DataFrames for each split\n",
    "df_train = df_combined[df_combined['Person_ID_Number'].isin(train_ids)]\n",
    "df_val = df_combined[df_combined['Person_ID_Number'].isin(val_ids)]\n",
    "df_test = df_combined[df_combined['Person_ID_Number'].isin(test_ids)]\n",
    "\n",
    "print(\"\\nData Split Summary:\")\n",
    "print(f\"Total number of unique persons: {n_ids}\")\n",
    "print(f\"Training set: {len(train_ids)} persons, {len(df_train)} samples\")\n",
    "print(f\"Validation set: {len(val_ids)} persons, {len(df_val)} samples\")\n",
    "print(f\"Test set: {len(test_ids)} persons, {len(df_test)} samples\")\n",
    "\n",
    "# Verify no data leakage\n",
    "train_ids_set = set(df_train['Person_ID_Number'].unique())\n",
    "val_ids_set = set(df_val['Person_ID_Number'].unique())\n",
    "test_ids_set = set(df_test['Person_ID_Number'].unique())\n",
    "\n",
    "print(\"\\nChecking for data leakage:\")\n",
    "print(\"Overlap between train and val:\", len(train_ids_set & val_ids_set))\n",
    "print(\"Overlap between train and test:\", len(train_ids_set & test_ids_set))\n",
    "print(\"Overlap between val and test:\", len(val_ids_set & test_ids_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6013143",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "- **Jittering (Gaussian Noise):** \n",
    "  - Add random Gaussian noise to signals to simulate sensor noise and improve robustness\n",
    "  - Noise level controlled through standard deviation parameter (0.005 to 0.02)\n",
    "  - Applied only to training data to maintain validation/test integrity\n",
    "- **Time Warping:** \n",
    "  - Randomly stretch or compress signal segments to simulate speed variations\n",
    "  - Uses cubic spline interpolation for smooth warping\n",
    "  - Warping factor range: 0.8 to 1.2 (±20% time variation)\n",
    "  - Applied only to training data to prevent data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a52db58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying data augmentation to training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 368/368 [00:14<00:00, 25.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training set size: 1104 signals\n",
      "Original signals: 368\n",
      "Augmented signals: 736\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation Functions\n",
    "import scipy.interpolate as interp\n",
    "\n",
    "def apply_jitter(signal, noise_level=0.01):\n",
    "    \"\"\"Add Gaussian noise to the signal\"\"\"\n",
    "    noise = np.random.normal(0, noise_level, signal.shape)\n",
    "    return signal + noise\n",
    "\n",
    "def apply_time_warp(signal, warp_factor_range=(0.8, 1.2)):\n",
    "    \"\"\"Apply random time warping to the signal\"\"\"\n",
    "    orig_len = len(signal)\n",
    "    warp_factor = np.random.uniform(*warp_factor_range)\n",
    "    new_len = int(orig_len * warp_factor)\n",
    "    \n",
    "    # Create warped time points\n",
    "    old_time = np.arange(orig_len)\n",
    "    new_time = np.linspace(0, orig_len-1, new_len)\n",
    "    \n",
    "    # Apply warping to each channel\n",
    "    warped_signal = np.zeros((new_len, signal.shape[1]))\n",
    "    for i in range(signal.shape[1]):\n",
    "        interpolator = interp.CubicSpline(old_time, signal[:, i])\n",
    "        warped_signal[:, i] = interpolator(new_time)\n",
    "    \n",
    "    # Resample back to original length\n",
    "    if new_len != orig_len:\n",
    "        final_time = np.linspace(0, new_len-1, orig_len)\n",
    "        final_signal = np.zeros((orig_len, signal.shape[1]))\n",
    "        for i in range(signal.shape[1]):\n",
    "            interpolator = interp.CubicSpline(new_time, warped_signal[:, i])\n",
    "            final_signal[:, i] = interpolator(final_time)\n",
    "        return final_signal\n",
    "    \n",
    "    return warped_signal\n",
    "\n",
    "# Apply augmentation to training data only\n",
    "augmented_records = []\n",
    "print(\"Applying data augmentation to training set...\")\n",
    "\n",
    "# Keep original records\n",
    "augmented_records.extend(df_train.to_dict('records'))\n",
    "\n",
    "# Create augmented versions\n",
    "for idx, row in tqdm(df_train.iterrows(), total=len(df_train)):\n",
    "    original_signal = row['signal']\n",
    "    \n",
    "    # Create 2 augmented versions per signal\n",
    "    for i in range(2):\n",
    "        new_record = row.copy()\n",
    "        augmented_signal = original_signal.copy()\n",
    "        \n",
    "        # Apply jittering with random noise level\n",
    "        noise_level = np.random.uniform(0.005, 0.02)\n",
    "        augmented_signal = apply_jitter(augmented_signal, noise_level)\n",
    "        \n",
    "        # Apply time warping\n",
    "        augmented_signal = apply_time_warp(augmented_signal)\n",
    "        \n",
    "        # Update signal and mark as augmented\n",
    "        new_record['signal'] = augmented_signal\n",
    "        new_record['augmented'] = True\n",
    "        augmented_records.append(new_record)\n",
    "\n",
    "# Create new DataFrame with original and augmented data\n",
    "df_train = pd.DataFrame(augmented_records)\n",
    "print(f\"Final training set size: {len(df_train)} signals\")\n",
    "print(f\"Original signals: {len(df_train) // 3}\")\n",
    "print(f\"Augmented signals: {len(df_train) - len(df_train) // 3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499c4f86",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "- **Statistical Features:** Compute basic statistics (mean, standard deviation, min, max, range, median) for each signal channel to capture essential characteristics of the data.\n",
    "- **Derivative Features:** Calculate first and second derivatives (jerk and snap) of the signals to quantify dynamic changes and motion patterns.\n",
    "- **Sliding Window Segmentation:** Segment each signal into overlapping windows to extract localized features, improving the model's ability to detect temporal patterns.\n",
    "- **Delta Features:** Compute the change (delta) in feature values between consecutive windows for each file, capturing transitions and trends within the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b2ff58b-965c-46ef-b3fc-68e279c60841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Simplified Feature extraction helpers\n",
    "def compute_basic_stats(signal):\n",
    "    \"\"\"Compute only basic statistics without entropy or zero crossings\"\"\"\n",
    "    return {\n",
    "        \"mean\": np.mean(signal),\n",
    "        \"std\": np.std(signal),\n",
    "        \"min\": np.min(signal),\n",
    "        \"max\": np.max(signal),\n",
    "        \"range\": np.max(signal) - np.min(signal),\n",
    "        \"median\": np.median(signal),\n",
    "    }\n",
    "\n",
    "def compute_derivatives(signal, order=1, sampling_rate=1000):\n",
    "    \"\"\"Compute derivatives up to snap (2nd derivative)\"\"\"\n",
    "    dt = 1 / sampling_rate\n",
    "    derivative = signal.copy()\n",
    "    for _ in range(order):\n",
    "        derivative = np.gradient(derivative, dt)\n",
    "    return derivative\n",
    "\n",
    "def compute_mass(signal):\n",
    "    \"\"\"Compute motion mass (sum of absolute values)\"\"\"\n",
    "    return np.sum(np.abs(signal))\n",
    "\n",
    "def extract_features_from_window(window_data, sampling_rate=1000):\n",
    "    \"\"\"Simplified feature extraction without entropy or zero crossings\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    for i, channel in enumerate(channel_names):\n",
    "        signal = window_data[:, i]\n",
    "        stats = compute_basic_stats(signal)\n",
    "        for stat_name, stat_val in stats.items():\n",
    "            features[f\"{channel}_{stat_name}\"] = stat_val\n",
    "\n",
    "        # First and second derivatives\n",
    "        jerk = compute_derivatives(signal, order=1, sampling_rate=sampling_rate)\n",
    "        snap = compute_derivatives(signal, order=2, sampling_rate=sampling_rate)\n",
    "\n",
    "        # Only mass features\n",
    "        features[f\"{channel}_jerk_mass\"] = compute_mass(jerk)\n",
    "        features[f\"{channel}_snap_mass\"] = compute_mass(snap)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e64489a-dd7b-4b10-a37e-6a4e447939f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1104/1104 [00:29<00:00, 37.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:02<00:00, 28.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:01<00:00, 47.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature extraction complete:\n",
      "Training set shape: (34275, 62)\n",
      "Validation set shape: (3503, 61)\n",
      "Test set shape: (1994, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Enhanced Sliding window feature extraction with metadata\n",
    "window_size = 1000  # 1 second\n",
    "step_size = 500     # 50% overlap\n",
    "sampling_rate = 1000\n",
    "\n",
    "def extract_features_from_df(df):\n",
    "    features = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        signal = row[\"signal\"]\n",
    "        label = row[\"label\"]\n",
    "        file_name = row[\"file_name\"]\n",
    "        \n",
    "        # Get all metadata columns (excluding signal, label, and file_name)\n",
    "        metadata = {k: v for k, v in row.items() \n",
    "                   if k not in ['signal', 'label', 'file_name']}\n",
    "\n",
    "        if signal.shape[0] < window_size:\n",
    "            continue  # Skip very short signals\n",
    "\n",
    "        for start in range(0, signal.shape[0] - window_size + 1, step_size):\n",
    "            end = start + window_size\n",
    "            window_data = signal[start:end, :]\n",
    "\n",
    "            window_features = extract_features_from_window(window_data, sampling_rate)\n",
    "            window_features[\"label\"] = label\n",
    "            window_features[\"file_name\"] = file_name\n",
    "            window_features[\"start_index\"] = start\n",
    "            window_features[\"end_index\"] = end\n",
    "            \n",
    "            # Add all metadata to the window features\n",
    "            window_features.update(metadata)\n",
    "\n",
    "            features.append(window_features)\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "# Extract features for each split\n",
    "print(\"Extracting features for training set...\")\n",
    "df_features_train = extract_features_from_df(df_train)\n",
    "print(\"Extracting features for validation set...\")\n",
    "df_features_val = extract_features_from_df(df_val)\n",
    "print(\"Extracting features for test set...\")\n",
    "df_features_test = extract_features_from_df(df_test)\n",
    "\n",
    "print(\"\\nFeature extraction complete:\")\n",
    "print(f\"Training set shape: {df_features_train.shape}\")\n",
    "print(f\"Validation set shape: {df_features_val.shape}\")\n",
    "print(f\"Test set shape: {df_features_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c58d911e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding delta features to training set...\n",
      "Adding delta features to validation set...\n",
      "Adding delta features to test set...\n",
      "\n",
      "Final shapes:\n",
      "Training set: (34275, 102)\n",
      "Validation set: (3503, 101)\n",
      "Test set: (1994, 101)\n",
      "\n",
      "Columns with missing values in df_delta_train:\n",
      "Weight        1809\n",
      "Height        1809\n",
      "Surename     33531\n",
      "Forename     33531\n",
      "augmented    11425\n",
      "dtype: int64\n",
      "\n",
      "Columns with missing values in df_delta_val:\n",
      "Surename    3503\n",
      "Forename    3503\n",
      "dtype: int64\n",
      "\n",
      "Columns with missing values in df_delta_test:\n",
      "Weight       198\n",
      "Height       198\n",
      "Surename    1994\n",
      "Forename    1994\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Add delta features (Now preserves metadata columns)\n",
    "def add_delta_features(df):\n",
    "    delta_features = []\n",
    "    grouped = df.groupby(\"file_name\")\n",
    "\n",
    "    # Define known metadata columns (these are columns we know are not feature columns)\n",
    "    base_non_feature_cols = [\n",
    "    'label', 'file_name', 'start_index', 'end_index',\n",
    "    'Person_ID_Number', 'Age', 'Gender', 'Writing_Hand',\n",
    "    'Weight', 'Height', 'Smoker', 'Notice', 'Object',\n",
    "    'Object_Index', 'Pen', 'Samplerate', 'Time', 'Date',\n",
    "    'Comment', 'Surename', 'Forename'\n",
    "    ]\n",
    "    \n",
    "    # Identify potential metadata columns (columns that are constant within each file)\n",
    "    potential_metadata_cols = [\n",
    "        col for col in df.columns \n",
    "        if col not in base_non_feature_cols \n",
    "        and df.groupby('file_name')[col].nunique().max() == 1\n",
    "    ]\n",
    "    \n",
    "    # Combine base non-feature columns with metadata columns\n",
    "    non_feature_cols = base_non_feature_cols + potential_metadata_cols\n",
    "\n",
    "    for file_name, group in grouped:\n",
    "        group = group.sort_values(\"start_index\").reset_index(drop=True)\n",
    "        group_delta = group.copy()\n",
    "\n",
    "        # Identify feature columns (excluding non-feature columns)\n",
    "        feature_cols = [col for col in group.columns if col not in non_feature_cols]\n",
    "        \n",
    "        # Compute deltas only for feature columns\n",
    "        for col in feature_cols:\n",
    "            group_delta[f\"delta_{col}\"] = group[col].diff()\n",
    "\n",
    "        # Fill first row deltas with 0\n",
    "        delta_cols = [f\"delta_{col}\" for col in feature_cols]\n",
    "        group_delta.loc[0, delta_cols] = 0\n",
    "\n",
    "        delta_features.append(group_delta)\n",
    "\n",
    "    return pd.concat(delta_features, ignore_index=True)\n",
    "\n",
    "# Process each split\n",
    "print(\"Adding delta features to training set...\")\n",
    "df_delta_train = add_delta_features(df_features_train)\n",
    "print(\"Adding delta features to validation set...\")\n",
    "df_delta_val = add_delta_features(df_features_val)\n",
    "print(\"Adding delta features to test set...\")\n",
    "df_delta_test = add_delta_features(df_features_test)\n",
    "\n",
    "print(\"\\nFinal shapes:\")\n",
    "print(f\"Training set: {df_delta_train.shape}\")\n",
    "print(f\"Validation set: {df_delta_val.shape}\")\n",
    "print(f\"Test set: {df_delta_test.shape}\")\n",
    "\n",
    "# Show columns with missing values and how many missing in each\n",
    "print(\"\\nColumns with missing values in df_delta_train:\")\n",
    "print(df_delta_train.isna().sum()[df_delta_train.isna().sum() > 0])\n",
    "\n",
    "print(\"\\nColumns with missing values in df_delta_val:\")\n",
    "print(df_delta_val.isna().sum()[df_delta_val.isna().sum() > 0])\n",
    "\n",
    "print(\"\\nColumns with missing values in df_delta_test:\")\n",
    "print(df_delta_test.isna().sum()[df_delta_test.isna().sum() > 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
