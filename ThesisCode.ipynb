{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c04246-8ca2-4a81-a14a-2e587d18b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup (Updated channel names)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Base paths configuration\n",
    "base_paths = {\n",
    "    \"healthy\": r\"C:\\NewHandPD\\Healthy Signals\\Signal\",\n",
    "    \"patient\": r\"C:\\NewHandPD\\Patient Signals\\Signal\"\n",
    "}\n",
    "\n",
    "category_labels = {\n",
    "    \"healthy\": 0,\n",
    "    \"patient\": 1\n",
    "}\n",
    "\n",
    "# Updated channel names (excluding Microphone)\n",
    "channel_names = [\n",
    "    \"Fingergrip\", \"Axial_Pressure\",\n",
    "    \"Tilt_X\", \"Tilt_Y\", \"Tilt_Z\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "660a4a02-fb71-4284-8fa2-d894b500572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Enhanced File Parsing Function with Metadata Extraction\n",
    "def parse_signal_file(file_path, label):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    meta_info = {}\n",
    "    signal_start_idx = 0\n",
    "    in_meta_section = False\n",
    "\n",
    "    # Extract metadata\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        \n",
    "        if line == \"#<meta>\":\n",
    "            in_meta_section = True\n",
    "            continue\n",
    "        elif line == \"#</meta>\":\n",
    "            in_meta_section = False\n",
    "            signal_start_idx = i + 1\n",
    "            break\n",
    "            \n",
    "        if in_meta_section and line.startswith(\"#<\") and line.endswith(\">\"):\n",
    "            # Extract key-value pairs from metadata\n",
    "            try:\n",
    "                key = line[2:line.find(\">\")]  # Get text between #< and >\n",
    "                value = line[line.find(\">\")+1:-1]  # Get text between > and <\n",
    "                \n",
    "                # Convert numeric values to appropriate type\n",
    "                if value.isdigit():\n",
    "                    value = int(value)\n",
    "                elif value.replace('.', '', 1).isdigit():\n",
    "                    value = float(value)\n",
    "                elif value.lower() == 'true':\n",
    "                    value = True\n",
    "                elif value.lower() == 'false':\n",
    "                    value = False\n",
    "                elif value == '':\n",
    "                    value = None\n",
    "                    \n",
    "                meta_info[key] = value\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # Load signal data and drop first channel (Microphone)\n",
    "    signal_lines = lines[signal_start_idx:]\n",
    "    signal_array = np.loadtxt(signal_lines, delimiter=\"\\t\")\n",
    "    \n",
    "    # Keep only channels 1-5 (drop channel 0 - Microphone)\n",
    "    if signal_array.ndim == 1:\n",
    "        signal_array = signal_array[1:6].reshape(1, -1)  # For single-row signals\n",
    "    else:\n",
    "        signal_array = signal_array[:, 1:6]  # For multi-row signals\n",
    "\n",
    "    return signal_array, label, meta_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "460ddd17-8b78-4fd8-8b78-92829c0bf3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing healthy files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [00:04<00:00, 84.52it/s] \n",
      "100%|██████████| 421/421 [00:04<00:00, 84.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing patient files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 373/373 [00:08<00:00, 45.24it/s]\n",
      "100%|██████████| 373/373 [00:08<00:00, 45.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Main Processing (Now includes metadata in records)\n",
    "all_records = []\n",
    "sigMea_records = []\n",
    "sigSp_records = []\n",
    "\n",
    "for category, folder_path in base_paths.items():\n",
    "    label = category_labels[category]\n",
    "    print(f\"Processing {category} files...\")\n",
    "\n",
    "    for file_name in tqdm(os.listdir(folder_path)):\n",
    "        if not file_name.endswith(\".txt\"):\n",
    "            continue\n",
    "            \n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            signal_data, label_val, meta_info = parse_signal_file(file_path, label)\n",
    "\n",
    "            record = {\n",
    "                \"file_name\": file_name,\n",
    "                \"label\": label_val,\n",
    "                \"signal\": signal_data,\n",
    "                **meta_info  # Unpack all metadata into the record\n",
    "            }\n",
    "\n",
    "            all_records.append(record)\n",
    "            \n",
    "            # Filter for specific file prefixes\n",
    "            if file_name.startswith(\"sigMea\"):\n",
    "                sigMea_records.append(record)\n",
    "            elif file_name.startswith(\"sigSp\"):\n",
    "                sigSp_records.append(record)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "531aa740-e3ed-49fb-94b5-6215dcb6fdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "Total files processed: 792\n",
      "Files starting with 'sigMea': 264\n",
      "Files starting with 'sigSp': 264\n",
      "\n",
      "Metadata columns found:\n",
      "['Person_ID_Number', 'Age', 'Gender', 'Writing_Hand', 'Weight', 'Height', 'Smoker', 'Notice', 'Object', 'Object_Index', 'Pen', 'Samplerate', 'Time', 'Date', 'Comment', 'Surename', 'Forename']\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Create DataFrames (Now includes metadata columns)\n",
    "df_all = pd.DataFrame(all_records)\n",
    "df_sigMea = pd.DataFrame(sigMea_records)\n",
    "df_sigSp = pd.DataFrame(sigSp_records)\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Total files processed: {len(all_records)}\")\n",
    "print(f\"Files starting with 'sigMea': {len(sigMea_records)}\")\n",
    "print(f\"Files starting with 'sigSp': {len(sigSp_records)}\")\n",
    "\n",
    "# Show metadata columns we've extracted\n",
    "print(\"\\nMetadata columns found:\")\n",
    "print([col for col in df_all.columns if col not in ['file_name', 'label', 'signal']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41f1769b-e204-4cad-af58-fbe7e317cb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame shape: (528, 20)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Combine sigMea and sigSp DataFrames\n",
    "df_combined = pd.concat([df_sigMea, df_sigSp], ignore_index=True)\n",
    "print(f\"Combined DataFrame shape: {df_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a281ba84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Split Summary:\n",
      "Total number of unique persons: 61\n",
      "Training set: 42 persons, 368 samples\n",
      "Validation set: 9 persons, 80 samples\n",
      "Test set: 10 persons, 80 samples\n",
      "\n",
      "Checking for data leakage:\n",
      "Overlap between train and val: 0\n",
      "Overlap between train and test: 0\n",
      "Overlap between val and test: 0\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.5: Split data by Person_ID_Number\n",
    "import numpy as np\n",
    "\n",
    "# Extract unique Person IDs\n",
    "person_ids = df_combined['Person_ID_Number'].unique()\n",
    "\n",
    "# Random shuffle of Person IDs\n",
    "np.random.seed(42)  # for reproducibility\n",
    "np.random.shuffle(person_ids)\n",
    "\n",
    "# Calculate split indices\n",
    "n_ids = len(person_ids)\n",
    "train_idx = int(0.7 * n_ids)\n",
    "val_idx = int(0.85 * n_ids)\n",
    "\n",
    "# Split Person IDs into train, validation, and test sets\n",
    "train_ids = person_ids[:train_idx]\n",
    "val_ids = person_ids[train_idx:val_idx]\n",
    "test_ids = person_ids[val_idx:]\n",
    "\n",
    "# Create DataFrames for each split\n",
    "df_train = df_combined[df_combined['Person_ID_Number'].isin(train_ids)]\n",
    "df_val = df_combined[df_combined['Person_ID_Number'].isin(val_ids)]\n",
    "df_test = df_combined[df_combined['Person_ID_Number'].isin(test_ids)]\n",
    "\n",
    "print(\"\\nData Split Summary:\")\n",
    "print(f\"Total number of unique persons: {n_ids}\")\n",
    "print(f\"Training set: {len(train_ids)} persons, {len(df_train)} samples\")\n",
    "print(f\"Validation set: {len(val_ids)} persons, {len(df_val)} samples\")\n",
    "print(f\"Test set: {len(test_ids)} persons, {len(df_test)} samples\")\n",
    "\n",
    "# Verify no data leakage\n",
    "train_ids_set = set(df_train['Person_ID_Number'].unique())\n",
    "val_ids_set = set(df_val['Person_ID_Number'].unique())\n",
    "test_ids_set = set(df_test['Person_ID_Number'].unique())\n",
    "\n",
    "print(\"\\nChecking for data leakage:\")\n",
    "print(\"Overlap between train and val:\", len(train_ids_set & val_ids_set))\n",
    "print(\"Overlap between train and test:\", len(train_ids_set & test_ids_set))\n",
    "print(\"Overlap between val and test:\", len(val_ids_set & test_ids_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b2ff58b-965c-46ef-b3fc-68e279c60841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Simplified Feature extraction helpers\n",
    "def compute_basic_stats(signal):\n",
    "    \"\"\"Compute only basic statistics without entropy or zero crossings\"\"\"\n",
    "    return {\n",
    "        \"mean\": np.mean(signal),\n",
    "        \"std\": np.std(signal),\n",
    "        \"min\": np.min(signal),\n",
    "        \"max\": np.max(signal),\n",
    "        \"range\": np.max(signal) - np.min(signal),\n",
    "        \"median\": np.median(signal),\n",
    "    }\n",
    "\n",
    "def compute_derivatives(signal, order=1, sampling_rate=1000):\n",
    "    \"\"\"Compute derivatives up to snap (2nd derivative)\"\"\"\n",
    "    dt = 1 / sampling_rate\n",
    "    derivative = signal.copy()\n",
    "    for _ in range(order):\n",
    "        derivative = np.gradient(derivative, dt)\n",
    "    return derivative\n",
    "\n",
    "def compute_mass(signal):\n",
    "    \"\"\"Compute motion mass (sum of absolute values)\"\"\"\n",
    "    return np.sum(np.abs(signal))\n",
    "\n",
    "def extract_features_from_window(window_data, sampling_rate=1000):\n",
    "    \"\"\"Simplified feature extraction without entropy or zero crossings\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    for i, channel in enumerate(channel_names):\n",
    "        signal = window_data[:, i]\n",
    "        stats = compute_basic_stats(signal)\n",
    "        for stat_name, stat_val in stats.items():\n",
    "            features[f\"{channel}_{stat_name}\"] = stat_val\n",
    "\n",
    "        # First and second derivatives\n",
    "        jerk = compute_derivatives(signal, order=1, sampling_rate=sampling_rate)\n",
    "        snap = compute_derivatives(signal, order=2, sampling_rate=sampling_rate)\n",
    "\n",
    "        # Only mass features\n",
    "        features[f\"{channel}_jerk_mass\"] = compute_mass(jerk)\n",
    "        features[f\"{channel}_snap_mass\"] = compute_mass(snap)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e64489a-dd7b-4b10-a37e-6a4e447939f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 368/368 [00:09<00:00, 39.20it/s]\n",
      "100%|██████████| 368/368 [00:09<00:00, 39.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:02<00:00, 27.59it/s]\n",
      "100%|██████████| 80/80 [00:02<00:00, 27.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:01<00:00, 48.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature extraction complete:\n",
      "Training set shape: (11425, 61)\n",
      "Validation set shape: (3503, 61)\n",
      "Test set shape: (1994, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Enhanced Sliding window feature extraction with metadata\n",
    "window_size = 1000  # 1 second\n",
    "step_size = 500     # 50% overlap\n",
    "sampling_rate = 1000\n",
    "\n",
    "def extract_features_from_df(df):\n",
    "    features = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        signal = row[\"signal\"]\n",
    "        label = row[\"label\"]\n",
    "        file_name = row[\"file_name\"]\n",
    "        \n",
    "        # Get all metadata columns (excluding signal, label, and file_name)\n",
    "        metadata = {k: v for k, v in row.items() \n",
    "                   if k not in ['signal', 'label', 'file_name']}\n",
    "\n",
    "        if signal.shape[0] < window_size:\n",
    "            continue  # Skip very short signals\n",
    "\n",
    "        for start in range(0, signal.shape[0] - window_size + 1, step_size):\n",
    "            end = start + window_size\n",
    "            window_data = signal[start:end, :]\n",
    "\n",
    "            window_features = extract_features_from_window(window_data, sampling_rate)\n",
    "            window_features[\"label\"] = label\n",
    "            window_features[\"file_name\"] = file_name\n",
    "            window_features[\"start_index\"] = start\n",
    "            window_features[\"end_index\"] = end\n",
    "            \n",
    "            # Add all metadata to the window features\n",
    "            window_features.update(metadata)\n",
    "\n",
    "            features.append(window_features)\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "# Extract features for each split\n",
    "print(\"Extracting features for training set...\")\n",
    "df_features_train = extract_features_from_df(df_train)\n",
    "print(\"Extracting features for validation set...\")\n",
    "df_features_val = extract_features_from_df(df_val)\n",
    "print(\"Extracting features for test set...\")\n",
    "df_features_test = extract_features_from_df(df_test)\n",
    "\n",
    "print(\"\\nFeature extraction complete:\")\n",
    "print(f\"Training set shape: {df_features_train.shape}\")\n",
    "print(f\"Validation set shape: {df_features_val.shape}\")\n",
    "print(f\"Test set shape: {df_features_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c58d911e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding delta features to training set...\n",
      "Adding delta features to validation set...\n",
      "Adding delta features to validation set...\n",
      "Adding delta features to test set...\n",
      "Adding delta features to test set...\n",
      "\n",
      "Final shapes:\n",
      "Training set: (11425, 101)\n",
      "Validation set: (3503, 103)\n",
      "Test set: (1994, 103)\n",
      "\n",
      "Checking for NaNs:\n",
      "Training set NaNs: True\n",
      "Validation set NaNs: True\n",
      "Test set NaNs: True\n",
      "\n",
      "Final shapes:\n",
      "Training set: (11425, 101)\n",
      "Validation set: (3503, 103)\n",
      "Test set: (1994, 103)\n",
      "\n",
      "Checking for NaNs:\n",
      "Training set NaNs: True\n",
      "Validation set NaNs: True\n",
      "Test set NaNs: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Add delta features (Now preserves metadata columns)\n",
    "def add_delta_features(df):\n",
    "    delta_features = []\n",
    "    grouped = df.groupby(\"file_name\")\n",
    "\n",
    "    # Define known metadata columns (these are columns we know are not feature columns)\n",
    "    base_non_feature_cols = ['label', 'file_name', 'start_index', 'end_index']\n",
    "    \n",
    "    # Identify potential metadata columns (columns that are constant within each file)\n",
    "    potential_metadata_cols = [\n",
    "        col for col in df.columns \n",
    "        if col not in base_non_feature_cols \n",
    "        and df.groupby('file_name')[col].nunique().max() == 1\n",
    "    ]\n",
    "    \n",
    "    # Combine base non-feature columns with metadata columns\n",
    "    non_feature_cols = base_non_feature_cols + potential_metadata_cols\n",
    "\n",
    "    for file_name, group in grouped:\n",
    "        group = group.sort_values(\"start_index\").reset_index(drop=True)\n",
    "        group_delta = group.copy()\n",
    "\n",
    "        # Identify feature columns (excluding non-feature columns)\n",
    "        feature_cols = [col for col in group.columns if col not in non_feature_cols]\n",
    "        \n",
    "        # Compute deltas only for feature columns\n",
    "        for col in feature_cols:\n",
    "            group_delta[f\"delta_{col}\"] = group[col].diff()\n",
    "\n",
    "        # Fill first row deltas with 0\n",
    "        delta_cols = [f\"delta_{col}\" for col in feature_cols]\n",
    "        group_delta.loc[0, delta_cols] = 0\n",
    "\n",
    "        delta_features.append(group_delta)\n",
    "\n",
    "    return pd.concat(delta_features, ignore_index=True)\n",
    "\n",
    "# Process each split\n",
    "print(\"Adding delta features to training set...\")\n",
    "df_delta_train = add_delta_features(df_features_train)\n",
    "print(\"Adding delta features to validation set...\")\n",
    "df_delta_val = add_delta_features(df_features_val)\n",
    "print(\"Adding delta features to test set...\")\n",
    "df_delta_test = add_delta_features(df_features_test)\n",
    "\n",
    "print(\"\\nFinal shapes:\")\n",
    "print(f\"Training set: {df_delta_train.shape}\")\n",
    "print(f\"Validation set: {df_delta_val.shape}\")\n",
    "print(f\"Test set: {df_delta_test.shape}\")\n",
    "\n",
    "# Verify no NaNs in any split\n",
    "print(\"\\nChecking for NaNs:\")\n",
    "print(\"Training set NaNs:\", df_delta_train.isna().any().any())\n",
    "print(\"Validation set NaNs:\", df_delta_val.isna().any().any())\n",
    "print(\"Test set NaNs:\", df_delta_test.isna().any().any())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
